\relax 
\providecommand*\new@tpo@label[2]{}
\AC@reset@newl@bel
\bibstyle{alpha}
\providecommand \oddpage@label [2]{}
\citation{wang2018pixel2mesh}
\citation{wang2018pixel2mesh}
\citation{1087032}
\citation{10.1007978-981-13-0224-4_40}
\citation{817351}
\citation{Jakob2015Instant}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{13}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lol}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces High-level depiction of \emph  {points2mesh} neural network, transforming scanned point cloud data to watertight meshes.\relax }}{14}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:intro}{{1.1}{14}}
\citation{Groh2017}
\citation{vieira2014gumbel}
\citation{DBLP:journals/corr/KipfW16}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Background}{15}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lol}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{sec:background}{{2}{15}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Related Work}{17}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lol}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{sec:relatedwork}{{3}{17}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Traditional approaches}{17}}
\newlabel{classic_approaches}{{3.1}{17}}
\citation{Lorensen:1987:MCH:37402.37422}
\citation{817351}
\citation{Amenta:2001:PC:376957.376986}
\citation{Lorensen:1987:MCH:37402.37422}
\citation{Groh2017}
\citation{Kazhdan:2006:PSR:1281957.1281965}
\citation{Jakob2015Instant}
\citation{bukenberger2018hierarchical}
\citation{Shilane:2004:TPS}
\citation{7298801}
\citation{shapenet2015}
\citation{xiang2016objectnet3d}
\citation{lpt2013ikea}
\citation{Thingi10K}
\citation{Silberman:ECCV12}
\citation{sunrgb}
\citation{objectnn-shrec17}
\citation{dai2017scannet}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Reconstruction methods}{18}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Machine Learning based approaches}{18}}
\newlabel{ml_approaches}{{3.2}{18}}
\citation{Maturana2015VoxNet}
\citation{inproceedings}
\citation{SZB17a}
\citation{articlefusionnet}
\citation{DBLP:journals/corr/QiSNDYG16}
\citation{DBLP:journals/corr/BrockLRW16}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}3D datasets}{19}}
\newlabel{3ddatasets}{{3.2.1}{19}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Learning in 3D}{19}}
\newlabel{learn3d}{{3.2.2}{19}}
\citation{DBLP:journals/corr/QiSMG16}
\citation{qi2017pointnetplusplus}
\citation{DBLP:journals/corr/KlokovL17}
\citation{DBLP:journals/corr/abs-1801-07791}
\citation{PointGrid}
\citation{DBLP:journals/corr/abs-1803-10409}
\citation{DBLP:journals/corr/abs-1712-10215}
\citation{DBLP:journals/corr/QiSMG16}
\citation{qi2017pointnetplusplus}
\citation{DBLP:journals/corr/DaiCSHFN17}
\citation{Groh2017}
\citation{inproceedingsparse}
\citation{DBLP:journals/corr/abs-1710-07563}
\citation{feng2018meshnet}
\citation{Kalogerakis:2010:labelMeshes}
\citation{Sidi:2011:UCS:2024156.2024160}
\citation{Kalogerakis:2017:ShapePFCN}
\citation{cmrKanazawa18}
\citation{DBLP:journals/corr/abs-1711-10669}
\citation{wang2018pixel2mesh}
\citation{mvcTulsiani18}
\citation{DBLP:journals/corr/abs-1804-06032}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Learning 3D reconstruction}{20}}
\newlabel{transform}{{3.2.3}{20}}
\citation{wang2018pixel2mesh}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Graph-based surface reconstruction network}{21}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lol}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{sec:methods}{{4}{21}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Neural network structure}{21}}
\newlabel{networkconfig}{{4.1}{21}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}General system overview}{21}}
\newlabel{generalsystem}{{4.1.1}{21}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces General workflow of \emph  {points2mesh} network. $\mathcal  {N}_{recon}$ takes a pointcloud $\mathcal  {PC}$ and an initial mesh $\mathcal  {M}_i$ as input, deforms $\mathcal  {M}_i$ based on important features $v_i$ in $\mathcal  {PC}$ to compute an approximate mesh $\mathaccentV {hat}05E{\mathcal  {M}}$\relax }}{22}}
\newlabel{fig:generalconfig}{{4.1}{22}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Traditional way to train neural network. Ground truth data and input are separated. Input may be infered from ground truth data.\relax }}{23}}
\newlabel{fig:training1}{{4.2}{23}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Network training process}{23}}
\newlabel{trainings}{{4.1.2}{23}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.3}Flexconv feature extraction}{23}}
\newlabel{fconv}{{4.1.3}{23}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Alternative approach to train $\mathcal  {N}_{opt}$. Input and ground truth data are the same in this example.\relax }}{24}}
\newlabel{fig:training2}{{4.3}{24}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces $\mathcal  {N}_{flex}$ with point cloud $\mathcal  {PC}$ as input, computing features $v^j$ in three blocks of flexconv layers and a trailing flexpool layer After each convolutional block, weighted random sampling reduces the number of samples of $\mathcal  {PC}$ by a factor of four. Feature vectors $v^j$ are concatenated with their respective positions $x^j$ in a vector $\textbf  {v}$.\relax }}{24}}
\newlabel{fig:flexconv}{{4.4}{24}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces $\mathcal  {N}_{gcn}$ with initial mesh $\mathcal  {M}_i$ as input. Feature vector $\textbf  {v}$ is projected onto the mesh $\mathcal  {M}_i$ and deformed according to it. Then an unpooling layer increases the number of vertices in $\mathcal  {M}_i$. Projecting $\textbf  {v}$ onto the graph and deforming it afterward is repeated two more times.\relax }}{25}}
\newlabel{fig:gcn}{{4.5}{25}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.4}GCN mesh deformation}{25}}
\newlabel{gcnconv}{{4.1.4}{25}}
\citation{wang2018pixel2mesh}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces The \emph  {unpooling} layer of $\mathcal  {N}_{gcn}$ increases the number of vertices by introducing a new node between each neighboring node (\nobreakspace  {}white\nobreakspace  {}) in $\mathcal  {M}_i$. A new node (\nobreakspace  {}green\nobreakspace  {}) is added precisely in the middle of each old edge. Finally, the new nodes are connected, while also updating the neighborhood of the old nodes.\relax }}{26}}
\newlabel{fig:unpool}{{4.6}{26}}
\newlabel{gcnconv}{{4.1.4}{26}}
\newlabel{form:align}{{4.2}{26}}
\citation{wang2018pixel2mesh}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces Initial ellipsoid in relation to input pointcloud $\mathcal  {PC}$ before aligning on the left. After \emph  {align graph} layer on the right. Thus, eliminating positional bias and letting the mesh shrink during deformation, rather than letting it grow.\relax }}{27}}
\newlabel{fig:align}{{4.7}{27}}
\newlabel{featureproj}{{4.1.4}{27}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces \emph  {graph projection} layer, assigning feature vectors $\textbf  {v}^j$ to nodes in $\mathcal  {M}_i$ based on the distance to their nearest neighbor in $\mathcal  {PC}$.\relax }}{28}}
\newlabel{fig:proj}{{4.8}{28}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.5}Loss functions}{28}}
\newlabel{lossfuncs}{{4.1.5}{28}}
\newlabel{form:edge}{{4.4}{29}}
\newlabel{form:cos}{{4.5}{29}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.6}Network configurations $\mathcal  {C}$}{29}}
\newlabel{configurations}{{4.1.6}{29}}
\@writefile{lof}{\contentsline {figure}{\nu