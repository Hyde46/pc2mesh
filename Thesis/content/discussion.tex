\chapter{Discussion}
\label{sec:dicussion}
\todo{kurze einleitung und zusammenfassung}

\section{Pioneer network \emph{points2mesh}}
\subsection*{variations}

  The proposed supervised as well as \emph{un}supervised neural network $\mathcal{N}_{recon}$, and $\mathcal{N}_{opt}$ respectively, 
  reconstruct a tri-mesh from point cloud data. The underlying object to reconstruct has to be singular. Both versions of the neural 
  network are trained on multiple classes at the same time, producing watertight meshes.
  First, configurations $\mathcal{C}_i$ are considered. As already established in \ref{chapter:evaluation}, the main difference, visually and 
  numerically are when configurations $\mathcal{C}_1$, $\mathcal{C}_3$ and $\mathcal{C}_4$ are compared to $\mathcal{C}_2$. When only looking
  at evaluations of $d_m$, configuration $\mathcal{C}_2$ has far lower values than any other configuration, impartial to training with 
  $\mathcal{N}_{recon}$ or $\mathcal{N}_{opt}$. Tables \ref{} through \ref{} confirm this proposition. By extending the number of used
  samples for the reconstruction in the base mesh $\mathcal{M}_i$, with an extra \emph{unpooling} step to \char`\~10k vertices, instead of
  \char`\~2.4k vertices.  , the prediction may then display more detail. 
  However, considering visual predictions of $\mathcal{C}_2$ in contrast to configurations with only \char`\~2.4k vertices, it is evident, 
  that $\mathcal{C}_2$ is generally less smooth. Taking a look at figure \ref{}, not only is the body of the airplane relatively rough,
  though details are much better visible. In contrast to the other three configurations, $\mathcal{C}_2$ seems to be less subjectively 
  visual pleasing, while still representing the reconstructed object accurately.  Configurations $\mathcal{C}_1$, $\mathcal{C}_3$ and 
  $\mathcal{C}_4$ are generally smoother than $\mathcal{C}_4$ as seen in figures \ref{} and \ref{}, also holding true for either $\mathcal{N}_{recon}$ 
  and $\mathcal{N}_{opt}$.
  Evidently, this is due to changes in $\mathcal{N}_{gcn}$ of the neural network (~See figure \ref{}~). The smoothness may be recovered 
  with more extended training or with more \emph{hyper parameter tuning} for the normal cosine loss $l_{cos}$. Applications like accurate 
  collision detection, or preparing 3D printable objects, greatly benefit from smoother predictions as seen with configuration $\mathcal{C}_1$.
  However, improving it further for configuration $\mathcal{C}_2$ has not been examined notably in the context of this work.


  Furthermore, configuration $\mathcal{C}_1$ and $\mathcal{C}_3$ are pretty similar in itself. Their main difference beinfg $\mathcal{C}_1$ does not 
  incur the nearest neighbor coordinates during the \emph{feature projection} layer, thus not utilizing it as a feature in $\mathcal{N}_{gcn}$.
  Nonetheless, figures \ref{winglets} and \ref{other} show that they still learn about the same deformations for the same given input. However,
  $\mathcal{C}_1$ can represent some features of objects better than $\mathcal{C}_3$, like the winglets of the airplane in figure \ref{winglets}
  or empty space between the legs of the chair in figure \ref{chair}, while also maintaining lower values of $d_m$. $\mathcal{C}_1$ reconstructing
  objects better than $\mathcal{C}_3$, not only shows that the former learns a variation of nearest neighbor coordinates during the
  \emph{flex conv feature extraction} but extends these features further to achieve better predictions. For that reason, $\mathcal{C}_1$ seems to be the 
  superior choice of configuration in comparison to $\mathcal{C}_3$.


  Configuration $\mathcal{C}_4$ describes a more simple configuration for $\mathcal{N}_{recon}$ and $\mathcal{N}_{opt}$. While still maintaining rather
  low values for $d_m$, in addition to rather pleasing visual results, its predictions do not reach the same level as $\mathcal{C}_1$ visually, or numerically
  as $\mathcal{C}_2$ (~See tables \ref{}~). 
  Considering figure \ref{simple}, $\mathcal{C}_4$ seems to produce a little smoother results than $\mathcal{C}_1$, though not as big of a gap as $\mathcal{C}_1$ 
  to $\mathcal{C}_2$. However, similarly to configuration $\mathcal{C}_3$, suffering from forfeiting various details of the ground truth mesh, as seen in figure
  \ref{}. $\mathcal{C}_4$ is less complex, faster to train (~Around $10\%$~) and provides the fastest inference times (~See table \ref{}~). However, 
  these advantages do not compensate for losing out on some features, which $\mathcal{C}_1$ and $\mathcal{C}_2$ are able to reconstruct.

  By nature of the composition of the neural network, the \emph{genus} of the prediction is bound to be of the same \emph{genus} as the input mesh 
  $\mathcal{M}_i$. Thus, if the input mesh is of a spheroidal shape (~\emph{genus} 0~), the prediction always is of \emph{genus} 0 as well.
  Albeit considering an input point cloud with an underlying ground truth mesh of \emph{genus} $\geq 1$, the prediction does not change its \emph{genus}.
  Disparities in the \emph{genus} lead to the cladding of holes in the mesh, as seen in figure \ref{chair with hole}. The ground truth chair has a hole
  below its backrest just above the chair seat. During the deformation in $\mathcal{N}_{gcn}$, the sphere contracts tighter around the shape of the point
  cloud. Even if the loss function incentives the network not to put vertices in the empty space, their connecting edges still remain stretched over it.
  No matter how the initial sphere is deformed, the holes cannot be adequately represented. 
  Thus, as described in \ref{torus}, the initial shape was changed to a toroidal model of \emph{genus} 0. The metric evaluation of $d_m$ did not increase 
  a lot as seen in \ref{table:torus}. However, $\mathcal{N}_{gcn}$ does not distinguish between different input meshes $\mathcal{M}_i$. Hence no incentive
  exists to either keep the hole or even move the hole towards a point in the point cloud where one could exist in the ground truth model. After only a few 
  deformation steps, the hole collapses in on itself, consequently the difference between deforming a sphere or a torus (~Or any other object of a higher
  \emph{genus} for that fact~) is negligible. 
  Reconstructing objects in that way requires a better way to keep the hole in the mesh, as well as find a way to place it at the right spot~(~s~) overlayed 
  on the point cloud. Let alone learning if the underlying mesh even has a hole, or not.

  Generally, the biggest hurdle is the problem of stretching the deforming sphere over empty space in the ground truth mesh. Most prominently, 
  it is visible in reconstructions from point cloud depicting chairs. As seen in figure \ref{bad chair}, in between the legs of the chair, many 
  edges are stretched between vertices placed directly on the leg of the chair.
  Contrary to that reconstruction, figure \ref{good chair} depicts a better reconstruction of a chair, without stretched edges over empty space.
  During deformation, the initial mesh contracts inwards, fitting its surface against the input point cloud data. During this process, two neighboring 
  vertices may be moved towards two distinct semantic parts of the point cloud (~Illustrated in figure \ref{}~), which leads to the stretching problem. 
  Successfully solving this problem leads to a significant improvement visually as well as numerically in the prediction.
  Two ways of tackling this problem are practiced in \emph{points2mesh}.
  The first one is implicit. Since $\mathcal{N}_{gcn}$ split into a three-step deformation process, dangling edges gain an additional vertex during
  the \emph{unpooling} layer. The newly added vertex then may move towards the connecting surface between the appendages, where the previous dangling
  edge existed (~Illustrated in figure \ref{}~). This has to be supported by proper feature vectors learned by $\mathcal{N}_{flex}$.
  Alternatively, if the vertices are already placed, neighboring vertices with long edges have to be moved in such way, that they get closer again, 
  potentially leading to minimize that problem, which is facilitated by the edge length loss $l_{edge}$.
  Configurations $\mathcal{C}_1$ and $\mathcal{C}_2$ generally are able to   solve this more consistently than $\mathcal{C}_3$ and $\mathcal{C}_4$. 
  If $\mathcal{N}_{recon}$ is trained without $\mathcal{N}_{flex}$ by only propagating nearest neighbor coordinates in the \emph{feature projection layer},
  the stretching problem is much more prevalent, leading to the conclusion that $\mathcal{N}_{flex}$ is crucial in learning feature vectors which give
  the incentive to move vertices in such a way to solve the problem.
  Additionally, $\mathcal{N}_{recon}$ is also able to produce less of these edges than $\mathcal{N}_{opt}$, since the former has a lot more information
  to learn and infer information from than the latter. 

  On another note, two different ways of training have been proposed, one \emph{surpervised} $\mathcal{N}_{recon}$, and the other a form of
  \emph{unsupervised} $\mathcal{N}_{opt}$ training. 
  As seen in chapter \ref{chap:results}, while both approaches yield visually pleasing reconstructions, the \emph{supervised} generally 
  produces better results. Naturally, the \emph{supervised} training has up to 40 times more samples during the training process and thus 
  has more information with which it can be trained. However, $\mathcal{N}_{opt}$ is still a compelling case to consider, since often in
  real life situations, no ground truth data is available (~Range scanners on autonomous cars~). Assessing how good such a network reconstructs
  meshes based on somewhat limited training data, is therefore appealing. As shown in chapter \ref{chap:results}, $\mathcal{N}_{opt}$ is still
  quite capable, though not as good as $\mathcal{N}_{recon}$. 
  Additionally, in such conditions, noise is inevitable. Nonetheless, $\mathcal{N}_{recon}$ shows that for a noise level of $0.01$ (~Based on ground 
  truth data in $[-1,1]$~) reconstructions are still visually pleasing, though the metric results of $d_m$ suffer a little. 
  Similarly, the generalization capabilities of $\mathcal{N}_{recon}$ have been assessed in figures \ref{fig:8} and \ref{fig:9}. Despite never seeing
  data of the category bathtub or person, the reconstruction does not stick out from the trained categories. Thus, somewhat robust training and inference have been achieved. 

Finally, real point cloud and short time argument
  \todo{reale point cloud}

  \subsection*{Comparing alternatives}
  Reconstructing meshes from point cloud data has been researched for many years. Thus, many techniques have been developed over the years.
  In the context of this work, assuming traditional, non-machine learning approaches, \emph{BPA} and \cite{IFM} are chosen as a comparison.
  First, \emph{BPA} generally yields lower values than configuration $\mathcal{C}_1$ for $\mathcal{N}_{recon}$ . In contrast, \emph{IFM} has 
  way higher numerical values for $d_m$ in contrast to $\mathcal{N}_{recon}$ as well as \emph{BPA} (~As seen in tables \ref{} through \ref{}~).
  However, considering the visual evidence, the supposed better values for \emph{BPA} cannot be confirmed to be generally better.
  Regarding the \emph{BPA} reconstruction in figure \ref{bpa object}, the \emph{IFM} reconstruction in figure \ref{ifm object}, a clear difference
  can be seen in contrast to the reconstruction by \emph{points2mesh} in figure \ref{points2mesh}. At a low amount of samples (~256 samples~), 
  neither \emph{BPA} nor \emph{IFM} produces watertight meshes. Even worse, \emph{IFM} struggles to reconstruct the general frame of the object. 
  Increasing the number of samples supports both \emph{BPA} as well as \emph{IFM}, yet is not enough for these approaches to provide watertight meshes.
  Even considering a reconstruction based on 7500 samples, the reconstructions of either approach is still not manifold as seen in figure \ref{high detail}.
  Filling these holes would be a completely new, non-trivial problem in itself. Though, visually, these reconstructions approximate the ground truth better 
  than any configuration of \emph{points2mesh}. However, only by the innate capability of humans to fill out the holes and thus, inferring how the mesh would
  look, \emph{if} it did not have them.


  A crucial part of this work is the attempt to design a neural network for learning surface reconstruction from point cloud data. Thus,
  comparisons to deep learning based approaches, though not plenty, are even more critical than traditional ones. 
  Since other deep learning based approaches transforming point cloud data directly to meshes have yet to be published,  another close technique
  is chosen as a comparison, \emph{deep marching cubes}.
  \emph{DMC} however, operates on voxelized data as its input, but also produces meshes watertight meshes. These deep learning approaches for mesh 
  reconstructions are still new and therefore are not as technically mature as current non-deep learning based methods.
  Considering the reconstructions in figures \ref{}, \ref{} and \ref{}, expected features from the traditional \emph{marching cubes} algorithm can
  also be seen here. Blocky reconstructions, accompanied by predetermined slopes at corners, lend a distinct look to reconstructions by either 
  technique. In contrast, reconstructions from \emph{points2mesh} resemble a more organic look and have to possibility of representing more details.
  The nature of its voxelized data restricts the visual quality of predictions by \emph{DMC}. Though, the number of vertices is not bound to a set
  amount for each object, but rather the resolution of the voxel grid. While both methods produce watertight meshes, \emph{DMC} sometimes produces
  holes in the mesh where a solid surface is present in the ground truth mesh, as seen in figure \ref{} or \ref{}. Additionally, \emph{DMC} at 
  times adds unexplained \emph{blobs} of geometry in the prediction. A post-processing step could easily remove them. However, they are detrimental
  for applications which rely on the best possible representation for a given point cloud.
  Still, \emph{DMC} has some clear advantages over \emph{points2mesh}, as seen in figure \ref{sofa}. \emph{DMC} reconstructs objects with edges only 
  parallel to the cardinal axes better than any configuration of \emph{points2mesh}. However, introducing edges, distributed in any way through space,
  its limitation are evident as seen in figures \ref{plane1} and \ref{plane2}. For example, the wings of the plane are neither x-parallel nor y-parallel.
  Thus, \emph{DMC} introduces \emph{stair}-like representation of slopes in the mesh. emph{DMC} does not have any way to represent curves. \emph{points2mesh}
  works despite these preconditions, yet giving it an organic look.
  On top of that, \emph{DMC} does not work correctly if the samples are sparse. Both planes in figures \ref{} and \ref{} were inferred based on 1024 samples.
  When the number of samples is too low, \emph{DMC} fails to produce any geometry for parts of the mesh.
  In contrast, \emph{points2mesh} only needs very few samples to produce geometry at a given part of the point cloud.

\section{Conclusion}


\emph{points2mesh} describes the first\footnote{No other network was found which directly transforms points cloud data if singular objects to meshes as of \today} deep
learning based neural network, transforming point cloud data to tri-meshes. Excelling in reconstructing with low-resolution data,
 even generating watertight meshes with only 256 data points while also being capable of generalizing over objects it has never seen 
 during training.

However, its inherent structure is restrictive. By virtue of how $\mathcal{N}_{gcn}$ works with \emph{graph convolution}, the amount
 of vertices used for the reconstruction is predetermined before the training has even started. 

fuer erstes netzwerk dieser art trotzdem super

 is fast

 generelle figur von objekten bekommt es sehr gut holding


 kann mit aktuellen methoden bei low resolution super mit halten


\section{Outlook}

\begin{enumerate}
  \item first venture for pc to mesh
  \item loecher einfuegen durch neue layer
  \item on the fly mehr detail wo notwendig
  \item nur flexconv
  \item mehr data augmentation
  \item loss function
  \item neue tensorflow graphics library eroeffnet neue moeglichkeiten bestimmt
\end{enumerate}