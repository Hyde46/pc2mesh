\chapter{Related Work}
\label{sec:relatedwork}
%https://github.com/timzhang642/3D-Machine-Learning#3d_synthesis
Often in computer graphics, it is necessary to process three-dimensional real-world or digital objects for reconstruction, remeshing or analytical applications. There are many ways to acquire data of the surface of such geometry and more so many techniques to transform and augment that data to different representations. This often non-trivial task is crucial to further process the object in question in later stages of their respective pipeline. Over the years many representations of acquisition data formats and transformation methods, as well as target data formats, have accumulated.\\
In this chapter, various of these techniques and data formats are examined, of which some of them are used in this work as a vehicle for a novel data transformation routine.\\
Initially, classical approaches are examined in section \ref{classic_approaches} which do not rely on artificial intelligence or machine learning methods.
With the recent advances in machine learning, and more so deep learning, many new approaches have been developed and thus considered in this work.
Hence, machine learning based methods are reassessed in section \ref{ml_approaches} which rely on distinctive statistical features in their initial data format or the dataset itself, thus allowing for the transformation.
\section{Classical approaches}
\label{classic_approaches}
  In this chapter, classical approaches are examined which do not rely on machine
  learning to transform data of objects from one format to another. 

  In general, a three-dimensional object may be specified by its representation of the
  surface. This representation varies from unstructured point cloud data in three-dimensional space
  to a graph-based representation like triangle-/ or quad-meshes and even poly meshes
  with more than four neighbors per node. Information on the object's surface may also be described
  by three-dimensional discrete scalar field (~Voxels~).  Often the information on an object's
  surface is only partially defined, where some information may be missing. 
  This can be due to acquisition methods, where parts of the object are partially 
  concealed or recorded from insufficient many sides, or that information is not available.

  Starting from these representations of an object's surface, there are many methods 
  of transformation to reach another state of representation. 
  Transforming the data not only makes further processing easier
  but rather during their process, more data on the surface is 
  computed based on the given input data.

  This task is non-trivial since, from one representation of an object's
  surface, many reconstructions in the form of another representation 
  are possible. The amount of data of the object's surface can never
  be entirely perfect, as the resolution is chosen arbitrarily, thus leading to many reconstruction methods.
  In this section, some of the more important works on this subject of data transformation are examined, with 
  an overview of the different input and output data formats applicable. 

 \subsection{Reconstruction methods}
 Reconstructing surface information of objects has been worked on for many years, 
 dating back to the earliest approaches in 1987 \cite{Lorensen:1987:MCH:37402.37422}, 
 with the marching cubes algorithm. Most of these earlier concepts rely on local neighborhood 
 reconstruction mechanism. Either by starting with unstructured points in three-dimensional 
 space \cite{817351,Amenta:2001:PC:376957.376986}, discrete scalar fields \cite{Lorensen:1987:MCH:37402.37422} or structured light \cite{Groh2017}. Later work incorporated global 
 optimization techniques to achieve better results \cite{Kazhdan:2006:PSR:1281957.1281965}. Though, more recent work relying on local optimization
  operators again, with faster computing times and being able to handle a much higher capacity of input data \cite{Jakob2015Instant}. 
 Even though using normal orientations of PC data is typical, advanced reconstruction techniques with limited
  information of the original object are still present \cite{bukenberger2018hierarchical}.
 
  %One of the earliest, but still well-known algorithms, presented by
  %Lorensen et al. \cite{Lorensen:1987:MCH:37402.37422}, the marching cubes algorithm, reconstructs the surface information of an object as a
  %triangle mesh, given three-dimensional medical data. This data is provided in the form of
  %voxels. 
  %During the reconstruction process, the algorithm iterates through all voxels. For each of these positions, up to eight occupied positions of the voxel-grid can be considered. Furthermore, a reconstruction configuration is defined for every configuration of present/non-present positions in the neighborhood of the voxel. Thus, leading to $2^8 = (256)$ possible configurations, where 15 of them are unique, and the rest are generated by the rotation of the base configurations.  Every configuration reconstructs a number of polygons given one voxel configuration. Finally, all polygons created this way are merged into one mesh.
%
  %A more recent work by Groh et al.\cite{Groh2017} presents a way to digitalize real-world objects by projecting shifted sinusoid patterns by multiple projectors onto the object. This is captured by multiple cameras. By combining the shifted patterns and highly accurate created features, a precise bundle adjustment is performed. Thus, all projector-camera pairs for each surface point can be estimated and used to optimize the final depth information.
  %\todo{Actually not sure if this fits here}
%
  %By all means, digitalizing real-world objects not only revolves on structured light but also may make use of range scanners, which measure the depth from the recording device to a point on the surface of the object. Additionally, this depth information may be combined from recording from multiple views. Assembling this information yields unstructured three-dimensional point cloud data. 
%
  %Similar to the marching cubes algorithm, the work presented by 
  %Bernardini et al. \cite{817351}, the ball-pivoting algorithm, 
  %interpolates the surface of an object given the input data.
  %In contrast, the algorithm processes such a collection of unstructured 
  %three-dimensional points in space with their orientation on the
  %ground truth surface and returns a triangle mesh. Starting from
  %a seed triangle, for each edge of not already processed triangles
  %, a sphere with set radius $r$ revolves around it. If another point
  %of the input point cloud intersects with the sphere, a triangle is 
  %created out of the endpoints of the considered edge and the newly 
  %found point, thus creating a triangle mesh if every edge of each 
  %triangle has been considered. 
  %
%
  %Furthermore, the Poisson surface reconstruction, 
  %presented by Kazhdan et al. \cite{Kazhdan:2006:PSR:1281957.1281965}, also processes a
  %collection of unstructured points in three-dimensional
  %space with surface orientation. Solving the Poisson problem 
  %\cite{Genovese2006} assuming the isosurface of the surface can be 
  %approximated by the normal field, thus yielding the
  %indicator function. This function results in non-zero
  %values close to the surface of the object and thereby 
  %can be directly extracted. The problem is then reduced to a 
  %form of voxel-grid, thus can be transformed by the marching
  %cubes algorithm.
%
  %Similarly, if not only samples on the surface are provided, 
  %but also their orientation, a high-resolution quad- or 
  %tri-dominant mesh can be reconstructed, as described in the 
  %work by Jakob et al. \cite{Jakob2015Instant}. Although not limited to PC data,
  %it is able to process from a hundred thousand up to several
  %hundred million data points in only seconds of calculation time.
  %While other methods rely on global optimization problems, this work 
  %specifically does not rely on such methods, scaling linearly with 
  %the data input size, thus leading to such fast calculation times.
%
  %Even though many different techiniques on surface scanning have been
  %developed over the years, obtaining the sampled surface point orientations,
  %connectivity or even topological adjacency is not always possible.
%
  %
  %Without any of these sometimes crucial information, surface information still can be reconstructed by
  %calculating the PC's medial axis transform ~(~MAT~) from unstructured point cloud
  %data, followed by an inverse transformation.  The medial axis of a PC
  %is described as a set of points where each point is the closest neighbor 
  %to at least one other point in the original PC. Not only can the PC be noisy 
  %or scanty, as described by Amenta et al. \cite{Amenta:2001:PC:376957.376986}, but the reconstructed
  %surface mesh will always be manifold.
%
  %In more recent work by Bukenberger et al. \cite{bukenberger2018hierarchical},
  %reconstruction of manifold quad faced meshes from 
  %scanned three-dimensional geometry without additional
  %information on surface orientation, connectivity or topological
  %adjacency is explored. Like in the previously described work,
  %differing sampling densities on the surface procure no severe 
  %limitation during the reconstruction process. This process takes
  %advantage of clustering nearby points, and computing 
  %representatives for each of them with the help of a kd-tree 
  %leaf structure. Thus, leading to a coherent tile structure 
  %of the surface. 
%
%
%
\section{Machine Learning based approaches}
\label{ml_approaches}
With more advanced acquisition methods, higher computing power 
and high-speed connectivity over the internet, gathering, interchanging
 and expanding big datasets are getting progressively more prevalent
  and important. 
  Thus, allowing for statistical and data-driven algorithms to 
  analyze big datasets, computing characteristics and similarities
   within them. 
   
  In this section, specfically machine learning based approaches are inspected which learn features from big
  datasets and utilize gained knowledge to infer information on an object's surface given varied input data.

  First, subsection \ref{3ddatasets} compiles an overview of the work on datasets, consisting of polygonal object data, range data, and object annotations
  to enable learning in three dimensional structures at all.
  Then, subsection \ref{learn3d}  current and older approaches to processing 
  and learning from three-dimensional data, independently on their input or output data
  format. 
  Furthermore, subsection \ref{transform} establishes a frame for this work in 
  contrast to the current, as well as older techniques for data 
  transformation from diverse object datasets to a final reconstruction
  of surface information.

  \subsection{3D datasets}
  \label{3ddatasets} 
  In recent years, a tremendous amount of work has been made in the field of learning in three-dimensional space.
  Still, different objectives on learning goals require different datasets to work on. Some of them are more general 
  set of objects, defined by tri- or quad meshes. Many of these datasets have been developed and published in the last 10 years.
   Exemplary by Shilane et al. \cite{Shilane:2004:TPS}, offering ~1800 meshes, with a rendered image of the object and object label. 
   Similarly, proposed in a paper by Wu et al.\cite{7298801}, ModelNet is a collection of
    clean CAD models, with complete polygonal mesh, textures, labels out of currently 662 model categories and 127,915 models\footnote{As of 1.6.2019}.
    Wu et al. also provide a much smaller and well-arranged subset with only 40 or 10 model categories where each object is axis aligned. 
    Furthermore, Chang et al. \cite{shapenet2015} provide an even bigger and richer annotated dataset of more than three million polygonal 
    mesh models and more than four thousand categories. While this dataset annotates objects in minuscule detail and semanticism, it also 
    offers a condensed version of the dataset, ShapenetCore, for a better digest of all objects with 51300 models and 55 categories. Meanwhile also offering a more 
    densely annotated subset, ShapeNetSem, but also PartNet\footnote{As of March, 2019}, a hierarchical part annotation subset of ShapeNetCore.
    Xiang et al. \cite{xiang2016objectnet3d} also describe a large scale dataset consisting of 100 categories, 90000 images, 200000 objects in the
     images of which are  44000 modeled, working towards facilitating object recognition and pose estimation of multiple objects in a scene. 
Lim et al. \cite{lpt2013ikea} propose different dataset of polygonal objects with images of each object for the purpose of better pose estimations.
Moreover, Zhou et al. \cite{Thingi10K} introduce an comprehensive dataset of 10000 3D-printing Models, 
providing a basis for structural and semantic analysis.
For different three-dimensional learning tasks, complete scenery is needed 
consisting of a collection of depth information, semantic labels, meshes,
 orientations, top-down 2D views or images, as proposed by \cite{Silberman:ECCV12,sunrgb,objectnn-shrec17,dai2017scannet}%,Matterport3D,savva2017minos,wu2018building,ai2thor,qiu2017unrealcv,xiazamirhe2018gibsonenv,InteriorNet18,hackel2017isprs}
 \subsection{Learning in 3D}
 \label{learn3d}
 Learning in three-dimensional space, especially in the context of neural network and deep learning, has recently
  started to gain traction and produce results. Still, a tremendous amount of work has been achieved. 
 As there are numerous different applications, goals, and intentions for learning in three-dimensional space, many 
 datasets in various data formats to learn on have been established. Most of which originate from range scans, surface samples,
  or polygonal mesh structures. 
 
 Since a collection of unregulated points in space like Point clouds are hard to structure, many opt to lessen the 
 complexity by structuring them in a voxel-based grid, thus reducing the resolution of the problem size.
 With this volumetric approach, various objectives have been worked on, and many results produced. Maturana et al.
  \cite{Maturana2015VoxNet}, as well as Wu et al.\cite{inproceedings} and Sedaghat et al. \cite{SZB17a}, show the possibility of learning object recognition 
  based on voxel data. Others use joint or multiple data formats, not only relying on one form of volumetric data. Consequently, 
  learning object representations in lower dimensional vectors, and thus allowing for object discrimination as well 
  \cite{articlefusionnet,DBLP:journals/corr/QiSNDYG16,DBLP:journals/corr/BrockLRW16}.

  Furthermore, expanding the resolution of objects by considering point cloud data, learning features for object 
  recognition is still possible, as shown by
   \cite{DBLP:journals/corr/QiSMG16,qi2017pointnetplusplus,DBLP:journals/corr/KlokovL17,DBLP:journals/corr/abs-1801-07791,PointGrid}. Additionally,
    semantic segmentation of such objects and 
  scenes are possible. While not only bound to voxelized data \cite{DBLP:journals/corr/abs-1803-10409,DBLP:journals/corr/abs-1712-10215}, point cloud data \cite{DBLP:journals/corr/QiSMG16,qi2017pointnetplusplus,DBLP:journals/corr/DaiCSHFN17,Groh2017,inproceedingsparse,DBLP:journals/corr/abs-1710-07563}, but also explicit surface
   definitions like polygonal meshes \cite{feng2018meshnet,Kalogerakis:2010:labelMeshes,Sidi:2011:UCS:2024156.2024160,Kalogerakis:2017:ShapePFCN}.

 \subsection{Learning 3D reconstruction}
 \label{transform}
 Learning 3D reconstruction can be quite a challenging task.
  Numerous and complex approaches have been tested to infer surface 
  structure from diverse input training data. Many of which rely 
  on single-image or multiple-image to polygonal mesh transformations. 
 While some try to reconstruct similar, already learned objects of
  specific categories from RGB images, others try to generalize this 
  step for all objects. 
 
 Naturally, object-specific categories are more restrictive in
  inference as well as generalization. Whereas works by Pontes et al.
   and Kong et al. rely on first finding a specific object, close to
    the input data. Then they deform this general class object to a 
    target object, expectedly close to the input data \cite{cmrKanazawa18,pontes2017image2mesh,inproceedingsLocal}. Nevertheless,
     work by Wang et al. \cite{wang2018pixel2mesh} demonstrate a general case of 
     reconstruction from RGB Input images.
 Combining multi-view depth images of objects as a basis for
  reconstruction is considered as well \cite{mvcTulsiani18}.
  While not restricted to reconstructing from images, 
  voxelized three-dimensional data also is utilized
   \cite{DBLP:journals/corr/abs-1804-06032} with its drawbacks of native lower resolution.
   No work in the field is present, as far as transforming point cloud data directly to a polygonal mesh structure.
   