\relax 
\providecommand*\new@tpo@label[2]{}
\AC@reset@newl@bel
\bibstyle{alpha}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{13}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lol}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Problem Statement}{13}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}chapter recapitulation}{13}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Background}{15}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lol}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{sec:background}{{2}{15}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Machine Learning review}{15}}
\newlabel{back:ptt}{{2.1}{15}}
\newlabel{ml_review}{{2.1}{15}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}stuff}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Weighted reservoir sampling}{15}}
\newlabel{subsec:wrs}{{2.2.1}{15}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Related Work}{17}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lol}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{sec:relatedwork}{{3}{17}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Traditional approaches}{17}}
\newlabel{classic_approaches}{{3.1}{17}}
\citation{Lorensen:1987:MCH:37402.37422}
\citation{817351}
\citation{Amenta:2001:PC:376957.376986}
\citation{Lorensen:1987:MCH:37402.37422}
\citation{Groh2017}
\citation{Kazhdan:2006:PSR:1281957.1281965}
\citation{Jakob2015Instant}
\citation{bukenberger2018hierarchical}
\citation{Shilane:2004:TPS}
\citation{7298801}
\citation{shapenet2015}
\citation{xiang2016objectnet3d}
\citation{lpt2013ikea}
\citation{Thingi10K}
\citation{Silberman:ECCV12}
\citation{sunrgb}
\citation{objectnn-shrec17}
\citation{dai2017scannet}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Reconstruction methods}{18}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Machine Learning based approaches}{18}}
\newlabel{ml_approaches}{{3.2}{18}}
\citation{Maturana2015VoxNet}
\citation{inproceedings}
\citation{SZB17a}
\citation{articlefusionnet}
\citation{DBLP:journals/corr/QiSNDYG16}
\citation{DBLP:journals/corr/BrockLRW16}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}3D datasets}{19}}
\newlabel{3ddatasets}{{3.2.1}{19}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Learning in 3D}{19}}
\newlabel{learn3d}{{3.2.2}{19}}
\citation{DBLP:journals/corr/QiSMG16}
\citation{qi2017pointnetplusplus}
\citation{DBLP:journals/corr/KlokovL17}
\citation{DBLP:journals/corr/abs-1801-07791}
\citation{PointGrid}
\citation{DBLP:journals/corr/abs-1803-10409}
\citation{DBLP:journals/corr/abs-1712-10215}
\citation{DBLP:journals/corr/QiSMG16}
\citation{qi2017pointnetplusplus}
\citation{DBLP:journals/corr/DaiCSHFN17}
\citation{Groh2017}
\citation{inproceedingsparse}
\citation{DBLP:journals/corr/abs-1710-07563}
\citation{feng2018meshnet}
\citation{Kalogerakis:2010:labelMeshes}
\citation{Sidi:2011:UCS:2024156.2024160}
\citation{Kalogerakis:2017:ShapePFCN}
\citation{cmrKanazawa18}
\citation{DBLP:journals/corr/abs-1711-10669}
\citation{wang2018pixel2mesh}
\citation{mvcTulsiani18}
\citation{DBLP:journals/corr/abs-1804-06032}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Learning 3D reconstruction}{20}}
\newlabel{transform}{{3.2.3}{20}}
\citation{wang2018pixel2mesh}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Methods}{21}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lol}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{sec:methods}{{4}{21}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Neural network structure}{21}}
\newlabel{networkconfig}{{4.1}{21}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}General system overview}{21}}
\newlabel{generalsystem}{{4.1.1}{21}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces General workflow of \emph  {points2mesh} network. $\mathcal  {N}_{recon}$ takes a pointcloud $\mathcal  {PC}$ and an initial mesh $\mathcal  {M}_i$ as input, deforms $\mathcal  {M}_i$ based on important features $v_i$ in $\mathcal  {PC}$ to compute an approximate mesh $\mathaccentV {hat}05E{\mathcal  {M}}$\relax }}{22}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:generalconfig}{{4.1}{22}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces $\mathcal  {N}_{flex}$ with point cloud $\mathcal  {PC}$ as input, computing features $v^j$ in three blocks of flexconv layers and a trailing flexpool layer After each convolutional block, weighted random sampling reduces the number of samples of $\mathcal  {PC}$ by a factor of four. Feature vectors $v^j$ are concatenated with their respective positions $x^j$ in a vector $\textbf  {v}$.\relax }}{23}}
\newlabel{fig:flexconv}{{4.2}{23}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Flexconv feature extraction}{23}}
\newlabel{fconv}{{4.1.2}{23}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.3}GCN mesh deformation}{23}}
\newlabel{gcnconv}{{4.1.3}{23}}
\citation{wang2018pixel2mesh}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces $\mathcal  {N}_{gcn}$ with initial mesh $\mathcal  {M}_i$ as input. Feature vector $\textbf  {v}$ is projected onto the mesh $\mathcal  {M}_i$ and deformed according to it. Then an unpooling layer increases the number of vertices in $\mathcal  {M}_i$. Projecting $\textbf  {v}$ onto the graph and deforming it afterward is repeated two more times.\relax }}{24}}
\newlabel{fig:gcn}{{4.3}{24}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces The \emph  {unpooling} layer of $\mathcal  {N}_{gcn}$ increases the number of vertices by introducing a new node between each neighboring node (\nobreakspace  {}white\nobreakspace  {}) in $\mathcal  {M}_i$. A new node (\nobreakspace  {}green\nobreakspace  {}) is added precisely in the middle of each old edge. Finally, the new nodes are connected, while also updating the neighborhood of the old nodes.\relax }}{24}}
\newlabel{fig:unpool}{{4.4}{24}}
\newlabel{gcnconv}{{4.1.3}{24}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces Initial ellipsoid in relation to input pointcloud $\mathcal  {PC}$ before aligning on the left. After \emph  {align graph} layer on the right. Thus, eliminating positional bias and letting the mesh shrink during deformation, rather than letting it grow.\relax }}{25}}
\newlabel{fig:align}{{4.5}{25}}
\newlabel{form:align}{{4.2}{25}}
\newlabel{featureproj}{{4.1.3}{25}}
\citation{wang2018pixel2mesh}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces \emph  {graph projection} layer, assigning feature vectors $\textbf  {v}^j$ to nodes in $\mathcal  {M}_i$ based on the distance to their nearest neighbor in $\mathcal  {PC}$.\relax }}{26}}
\newlabel{fig:proj}{{4.6}{26}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.4}Loss functions}{27}}
\newlabel{lossfuncs}{{4.1.4}{27}}
\newlabel{form:edge}{{4.4}{27}}
\newlabel{form:cos}{{4.5}{27}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.5}Network configurations $\mathcal  {C}$}{27}}
\newlabel{configurations}{{4.1.5}{27}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces Baseconfiguration $\mathcal  {C}_1$ of $\mathcal  {N}_{recon}$.\relax }}{28}}
\newlabel{fig:c1}{{4.7}{28}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces Configuration $\mathcal  {C}_2$ of $\mathcal  {N}_{recon}$, showing its difference in $\mathcal  {N}_{gcn}$ to configuration $\mathcal  {C}_1$. $\mathcal  {C}_2$ introduces an extra \emph  {projection}, \emph  {unpooling} layer and a \emph  {graph convolution} block. Thus, allowing for a higher detailed approximate mesh $\mathaccentV {hat}05E{\mathcal  {M}}$ \relax }}{28}}
\newlabel{fig:c2}{{4.8}{28}}
\